---
layout: post
title: "Thursday, Jun 16"
date: 2016-06-16
categories: [research, usra]
tags: [computer vision, machine learning]
---
#### To do
- Complete notes for lectures 12 (Kernels and SVM) and start writing notes for lectures 13
- Start watching Andrew Ng lectures 17 (skip 16)
	- Learning With Large Datasets
	- Stochastic Gradient Descent
	- Mini-Batch Gradient Descent
	- Stochastic Gradient Descent Convergence
	- Online Learning
- Read Karpathy blog [post](http://karpathy.github.io/2016/05/31/rl/) on reinforcement learning in preparation for Adam's whiteboard lecture on it tomorrow

#### Done
- Watched Andrew Ng lectures 17 up to and not including the end, which is about map reduce and data parallelism
- Read Karpathy blog [post](http://karpathy.github.io/2016/05/31/rl/) on Policy Gradients and read the Nervana [post](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) on Q-Learning
	- All of this is under the field of Reinforcement Learning, which is what we'll be using to compete in the upcoming [VizDoom](http://vizdoom.cs.put.edu.pl/competition-cig-2016) competition
- Skipped writing notes for lecture 12 since Andrew Ng doesn't do such a good job in covering them. Instead, I'm using [this](https://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=Lectures.Kernels) post on kernels as a learning resource

#### Extras
- [Playing Atari With Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602v1.pdf)
- [ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning](http://arxiv.org/pdf/1605.02097v1.pdf)
- Forgot to mention that I bumped into this cool paper a couple of days ago: [Convolutional Sketch Inversion](http://arxiv.org/pdf/1606.03073v1.pdf). I'd definitely want to read through this soon. Currently held back by note taking the Andrew Ng lectures.
